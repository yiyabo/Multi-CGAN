#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†ææœ€ç»ˆè®­ç»ƒç»“æœ
"""

import numpy as np
import matplotlib.pyplot as plt
import os
from collections import Counter
import torch

def analyze_final_results():
    """åˆ†ææœ€ç»ˆè®­ç»ƒç»“æœ"""
    print("ğŸ‰ === è®­ç»ƒå®Œæˆï¼æœ€ç»ˆç»“æœåˆ†æ === ğŸ‰\n")
    
    results_dir = "gram_negative_results_server"
    
    # 1. æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
    print("ğŸ“ æ–‡ä»¶æ£€æŸ¥:")
    required_files = [
        "final_model.pth",
        "checkpoint_epoch_1500.pth", 
        "generated_epoch_1500.npy",
        "loss_history.npy"
    ]
    
    for file in required_files:
        file_path = f"{results_dir}/{file}"
        if os.path.exists(file_path):
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"  âœ… {file} ({size:.2f} MB)")
        else:
            print(f"  âŒ {file} (ç¼ºå¤±)")
    
    # 2. åˆ†ææŸå¤±å†å²
    print(f"\nğŸ“Š æŸå¤±åˆ†æ:")
    try:
        loss_data = np.load(f"{results_dir}/loss_history.npy", allow_pickle=True).item()
        
        if 'generator' in loss_data and 'discriminator' in loss_data:
            gen_losses = loss_data['generator']
            disc_losses = loss_data['discriminator']
            
            print(f"  è®­ç»ƒè½®æ•°: {len(gen_losses)}")
            print(f"  GeneratoræŸå¤±:")
            print(f"    æœ€ç»ˆ: {gen_losses[-1]:.4f}")
            print(f"    æœ€ä½³: {min(gen_losses):.4f}")
            print(f"    å¹³å‡: {np.mean(gen_losses):.4f}")
            
            print(f"  DiscriminatoræŸå¤±:")
            print(f"    æœ€ç»ˆ: {disc_losses[-1]:.4f}")
            print(f"    æœ€ä½³: {min(disc_losses):.4f}")
            print(f"    å¹³å‡: {np.mean(disc_losses):.4f}")
            
            # æ”¶æ•›åˆ†æ
            if len(gen_losses) > 100:
                recent_gen = np.mean(gen_losses[-100:])
                early_gen = np.mean(gen_losses[:100])
                improvement = ((early_gen - recent_gen) / abs(early_gen)) * 100
                
                if improvement > 0:
                    print(f"  ğŸ“ˆ Generatoræ”¹å–„: {improvement:.1f}%")
                else:
                    print(f"  ğŸ“‰ Generatorå˜åŒ–: {improvement:.1f}%")
        
    except Exception as e:
        print(f"  âŒ æ— æ³•è¯»å–æŸå¤±å†å²: {e}")
    
    # 3. åˆ†æç”Ÿæˆæ ·æœ¬
    print(f"\nğŸ§¬ ç”Ÿæˆæ ·æœ¬åˆ†æ:")
    try:
        generated_samples = np.load(f"{results_dir}/generated_epoch_1500.npy")
        print(f"  æ ·æœ¬æ•°é‡: {len(generated_samples)}")
        
        # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
        amino_acids = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y',' ']
        
        sequences = []
        lengths = []
        
        for seq_indices in generated_samples:
            seq_str = ''.join([amino_acids[idx] for idx in seq_indices])
            seq_str = seq_str.rstrip(' ')  # ç§»é™¤padding
            if len(seq_str) > 0:
                sequences.append(seq_str)
                lengths.append(len(seq_str))
        
        valid_sequences = len(sequences)
        print(f"  æœ‰æ•ˆåºåˆ—: {valid_sequences}/{len(generated_samples)} ({valid_sequences/len(generated_samples)*100:.1f}%)")
        
        if lengths:
            print(f"  åºåˆ—é•¿åº¦ç»Ÿè®¡:")
            print(f"    å¹³å‡: {np.mean(lengths):.2f}")
            print(f"    èŒƒå›´: {min(lengths)}-{max(lengths)}")
            print(f"    æ ‡å‡†å·®: {np.std(lengths):.2f}")
            
            # é•¿åº¦åˆ†å¸ƒ
            length_counter = Counter(lengths)
            print(f"  æœ€å¸¸è§é•¿åº¦:")
            for length, count in length_counter.most_common(5):
                print(f"    {length}ä¸ªæ°¨åŸºé…¸: {count}æ¬¡ ({count/len(lengths)*100:.1f}%)")
            
            # æ°¨åŸºé…¸ç»„æˆåˆ†æ
            all_aa = ''.join(sequences)
            aa_counter = Counter(all_aa)
            
            print(f"  æ°¨åŸºé…¸ç»„æˆ (å‰10ä¸ª):")
            for aa, count in aa_counter.most_common(10):
                print(f"    {aa}: {count} ({count/len(all_aa)*100:.2f}%)")
            
            # æ˜¾ç¤ºæ ·æœ¬åºåˆ—
            print(f"\n  æ ·æœ¬åºåˆ— (å‰10ä¸ª):")
            for i, seq in enumerate(sequences[:10]):
                print(f"    {i+1:2d}: {seq}")
        
    except Exception as e:
        print(f"  âŒ æ— æ³•åˆ†æç”Ÿæˆæ ·æœ¬: {e}")
    
    # 4. ä¸åŸå§‹æ•°æ®å¯¹æ¯”
    print(f"\nğŸ”¬ ä¸åŸå§‹æ•°æ®å¯¹æ¯”:")
    try:
        # åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®è¿›è¡Œå¯¹æ¯”
        original_data = np.load("data/gram_negative_train_pos.npy")
        
        # è½¬æ¢åŸå§‹æ•°æ®ä¸ºåºåˆ—
        original_sequences = []
        original_lengths = []
        
        for seq_indices in original_data[:100]:  # å–å‰100ä¸ªæ ·æœ¬å¯¹æ¯”
            seq_str = ''.join([amino_acids[idx] for idx in seq_indices])
            seq_str = seq_str.rstrip(' ')
            if len(seq_str) > 0:
                original_sequences.append(seq_str)
                original_lengths.append(len(seq_str))
        
        if original_lengths and lengths:
            print(f"  é•¿åº¦å¯¹æ¯”:")
            print(f"    åŸå§‹æ•°æ®å¹³å‡é•¿åº¦: {np.mean(original_lengths):.2f}")
            print(f"    ç”Ÿæˆæ•°æ®å¹³å‡é•¿åº¦: {np.mean(lengths):.2f}")
            
            # æ°¨åŸºé…¸ç»„æˆå¯¹æ¯”
            original_aa = ''.join(original_sequences)
            original_aa_counter = Counter(original_aa)
            
            print(f"  æ°¨åŸºé…¸é¢‘ç‡å¯¹æ¯” (å‰5ä¸ª):")
            print(f"    {'AA':<3} {'åŸå§‹':<8} {'ç”Ÿæˆ':<8} {'å·®å¼‚':<8}")
            for aa, _ in original_aa_counter.most_common(5):
                orig_freq = original_aa_counter[aa] / len(original_aa) * 100
                gen_freq = aa_counter.get(aa, 0) / len(all_aa) * 100 if 'all_aa' in locals() else 0
                diff = gen_freq - orig_freq
                print(f"    {aa:<3} {orig_freq:<8.2f} {gen_freq:<8.2f} {diff:+.2f}")
        
    except Exception as e:
        print(f"  âš ï¸  æ— æ³•åŠ è½½åŸå§‹æ•°æ®è¿›è¡Œå¯¹æ¯”: {e}")
    
    # 5. ä¿å­˜åˆ†æç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ:")
    
    # ä¿å­˜ç”Ÿæˆçš„åºåˆ—ä¸ºFASTAæ ¼å¼
    if 'sequences' in locals() and sequences:
        fasta_file = f"{results_dir}/final_generated_peptides.fasta"
        with open(fasta_file, 'w') as f:
            for i, seq in enumerate(sequences):
                f.write(f">generated_peptide_{i+1}\n")
                f.write(f"{seq}\n")
        print(f"  âœ… FASTAæ–‡ä»¶: {fasta_file}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = f"{results_dir}/training_statistics.txt"
    with open(stats_file, 'w') as f:
        f.write("Multi-CGAN è®­ç»ƒç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 30 + "\n\n")
        f.write(f"è®­ç»ƒå®Œæˆæ—¶é—´: 2025-07-06 11:20:59\n")
        f.write(f"æ€»è®­ç»ƒè½®æ•°: 1500\n")
        f.write(f"GPU: RTX 4090 D\n")
        f.write(f"æ‰¹æ¬¡å¤§å°: 64\n\n")
        
        if 'gen_losses' in locals():
            f.write(f"æœ€ç»ˆGeneratoræŸå¤±: {gen_losses[-1]:.4f}\n")
            f.write(f"æœ€ç»ˆDiscriminatoræŸå¤±: {disc_losses[-1]:.4f}\n\n")
        
        if 'valid_sequences' in locals():
            f.write(f"ç”Ÿæˆçš„æœ‰æ•ˆåºåˆ—æ•°: {valid_sequences}\n")
            f.write(f"å¹³å‡åºåˆ—é•¿åº¦: {np.mean(lengths):.2f}\n")
            f.write(f"åºåˆ—é•¿åº¦èŒƒå›´: {min(lengths)}-{max(lengths)}\n")
    
    print(f"  âœ… ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")
    
    print(f"\nğŸŠ åˆ†æå®Œæˆï¼è®­ç»ƒéå¸¸æˆåŠŸï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {results_dir}/")
    print(f"ğŸ§¬ ç”Ÿæˆçš„è‚½æ®µåºåˆ—: {results_dir}/final_generated_peptides.fasta")

if __name__ == "__main__":
    analyze_final_results()
